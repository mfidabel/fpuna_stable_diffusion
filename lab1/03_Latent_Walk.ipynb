{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad9dda3-3937-485d-ba2c-fc41a6b8d1fe",
   "metadata": {},
   "source": [
    "# Lab 1: Introducci칩n a Stable Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed75b36-4d8b-4df6-b1a9-c514139ddc17",
   "metadata": {},
   "source": [
    "Referencias:\n",
    "\n",
    "- [Stable Diffusion Videos 游닣 / nateraw - GitHub ](https://github.com/nateraw/stable-diffusion-videos)\n",
    "- [A walk through latent space with Stable Diffusion - Keras](https://keras.io/examples/generative/random_walks_with_stable_diffusion/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f183285-ce84-4205-8aa9-3522a0db601c",
   "metadata": {},
   "source": [
    "## Parte 3: Generaci칩n de Videos con Stable Diffusion 游닣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6c2ea4-afeb-4fd1-b22d-e197d3bffe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers accelerate ftfy fpuna-stable-diffusion diffusers==0.14.0 av"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce05809-eb7d-4664-9c9e-8fb91c36d6a8",
   "metadata": {},
   "source": [
    "Los modelos de generaci칩n de im치genes aprenden una representaci칩n \"latente\" del mundo visual: un vector de baja dimensi칩n donde cada punto se puede convertir en una imagen. Ir desde ese punto latente a una imagen se llama \"decodificaci칩n\". En Stable Diffusion 游빋, esto es manejado por el \"decodificador\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e2b12-00c4-4e5d-b461-0f4013c665f4",
   "metadata": {},
   "source": [
    "Esta variedad latente de im치genes es continua e interpolativa, significando que:\n",
    "\n",
    "1. Moverse un poquito en el espacio latente hace que la imagen correspondiente se cambie un poquito (continuidad)\n",
    "2. Para cualquier dos puntos **A** y **B** (por ejemplo, cualquier dos im치genes), es posible moverse de **A** a **B** por un camino donde cada punto intermedio esta tambi칠n en el espacio latente (por ejemplo, tambi칠n otra imagen valida). Los puntos intermedios pueden ser llamados \"interpolaciones\" entre las dos im치genes de inicio. \n",
    "\n",
    "Stable Diffusion 游빋 no solo es un modelo de im치genes, sino tambi칠n, un modelo de lenguaje natural. Posee dos espacios latentes: el espacio de representaci칩n de imagen aprendida por el codificador durante el entrenamiento, y el espacio latente del texto que es aprendida usando la combinaci칩n de pre-entrenamiento y ajuste duante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b76896c-1d28-4540-9d6d-1650e2e905d0",
   "metadata": {},
   "source": [
    "_Latent space walking_, o, _exploraci칩n en el espacio latente_, es el proceso de obtener un punto de muestra en el espacio latente e ir cambiando la representaci칩n latente de forma incrementada. Su aplicaci칩n m치s com칰n es generar animaciones donde cada punto muestreado es alimentado al decodificador y es guardado como un fotograma en la animaci칩n final.\n",
    "\n",
    "Para representaciones latentes de alta calidad, esto produce animaciones que parecen coherentes. Estas animaciones pueden proveer una cierta intuici칩n del espacio latente, y que puede ultimamente llevar a mejoras en el proceso de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc535177-0094-414f-897d-aa2e47fd25ee",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\"><img src=\"https://keras.io/img/examples/generative/random_walks_with_stable_diffusion/panda2plane.gif\" alt=\"panda a avi칩n\"  width=\"300\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e29b38-437d-4c50-a109-cde39d4a33f0",
   "metadata": {},
   "source": [
    "### Creamos nuestro modelo de Stable Diffusion 游빋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2c7ee-4cf1-4c47-873d-8b826e1e8c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1e71b-7cd4-4931-9d21-8284b78f1834",
   "metadata": {},
   "source": [
    "### Interpolando entre los text prompts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5efdae8-69bf-4002-8c37-0a213a6074f4",
   "metadata": {},
   "source": [
    "En Stable Diffusion, un text prompt es primero codificado en un vector, y esa codificaci칩n es luego utilizada para guiar el proceso de difusi칩n. El vector latente tiene un tama침o de 77x768 (es una matriz!), y cuando damos a Stable Diffusion un text prompt, estamos generando im치genes desde solamente un punto en el espacio latente.\n",
    "\n",
    "Para explorar m치s este espacio latente, podemos interpolar entre dos codificaciones de texto y generar im치genes en esos puntos interpolados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e286b24-5a2a-4513-94f4-3d03cc5bdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "interpolation_steps = 5\n",
    "prompt_1 = \"A watercolor painting of a Golden Retriever at the beach\"\n",
    "prompt_2 = \"A still life DSLR photo of a bowl of fruit\"\n",
    "\n",
    "tokenized_1 = pipe.tokenizer(prompt_1, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "tokenized_2 = pipe.tokenizer(prompt_2, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "encoding_1 = pipe.text_encoder(tokenized_1.input_ids.to(\"cuda\"))[0][0].cpu().detach().numpy()\n",
    "encoding_2 = pipe.text_encoder(tokenized_2.input_ids.to(\"cuda\"))[0][0].cpu().detach().numpy()\n",
    "\n",
    "interpolated_encodings = np.linspace(encoding_1, encoding_2, interpolation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b910ae1-ef12-4cb4-b39b-1d00da7e03d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_encodings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e87d503-e6e3-433c-bf81-00707f0038d2",
   "metadata": {},
   "source": [
    "Una vez que hemos interpolado las codificaciones latentes, podemos generar im치genes desde cualquier punto. Nota que para mantener cierta estabilidad entre las im치genes resultantes, debemos mantener el ruido de difusi칩n constante entre las im치genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da93d3c-e38c-44d1-a3fc-e7fcfc7025b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(12345)\n",
    "noise = torch.randn((1, 4, 64, 64), generator=generator, dtype=torch.float16)\n",
    "noise = torch.cat([noise]*interpolation_steps, dim=0)\n",
    "\n",
    "images = pipe(prompt_embeds=torch.from_numpy(interpolated_encodings).to(\"cuda\"),\n",
    "              latents=noise\n",
    "             ).images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f48126-1a2b-4a5f-81dd-9eb6061218d3",
   "metadata": {},
   "source": [
    "Ahora que generamos algunas im치genes interpoladas, echemos un vistazo!\n",
    "\n",
    "Durante el laboratorio, vamos a estar exportando las secuencias de imagenes como gifs para que puedan verse facilmente con cierto contexto temporal. Para las secuencias de im치genes donde la primera y la ultima imagen no coincide conceptualmente, vamos hacer que se repita el video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff029139-087a-45de-80e4-900e1af1c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpuna_stable_diffusion.utils import image_grid\n",
    "\n",
    "image_grid(images, rows=1, cols=interpolation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98848a4a-124b-4d21-8c2c-a4a0555114d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_as_gif(filename, images, frames_per_second=10, rubber_band=False):\n",
    "    if rubber_band:\n",
    "        images += images[2:-1][::-1]\n",
    "    images[0].save(\n",
    "        filename,\n",
    "        save_all=True,\n",
    "        append_images=images[1:],\n",
    "        duration=1000 // frames_per_second,\n",
    "        loop=0,\n",
    "    )\n",
    "    \n",
    "export_as_gif(\n",
    "    \"doggo-and-fruit-5.gif\",\n",
    "    images,\n",
    "    frames_per_second=2,\n",
    "    rubber_band=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9558007d-fe80-4cde-9723-2188d471173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IImage\n",
    "IImage(\"doggo-and-fruit-5.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02e3262-231f-4e7b-9726-1480345a7f4c",
   "metadata": {},
   "source": [
    "Los resultados pueden parecer sorprendente. Generalmente, interpolar entre prompts produce im치genes coherentes, y demuestran muy de seguido un cambio de concepto progresivo entre los contenidos de los dos textos. Esto es indicativo de un espacio de representaci칩n de alta calidad, que refleja cercanamente la estructura natural del mundo visual.\n",
    "\n",
    "Para visualizar mejor, tenemos que hacer una interpolaci칩n mucho m치s fina, utilizando cientos de pasos de interpolaci칩n. Para poder mantener el tama침o de las interpolaciones baja (para evitar que la GPU se quede corto de memoria), se requiere lotear manualmente las interpolaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5c0410-1782-4fc1-bf50-f3fa8bfa56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolation_steps = 30\n",
    "batch_size = 3\n",
    "batches = interpolation_steps // batch_size\n",
    "\n",
    "# Generamos de nuevo el ruido\n",
    "generator = torch.Generator().manual_seed(12345)\n",
    "noise = torch.randn((1, 4, 64, 64), generator=generator, dtype=torch.float16)\n",
    "\n",
    "# Generamos las interpolaciones y la dividimos en lotes\n",
    "interpolated_encodings = torch.from_numpy(np.linspace(encoding_1, encoding_2, interpolation_steps))\n",
    "interpolated_encodings = interpolated_encodings.to(\"cuda\") # Hay que llevar a GPU\n",
    "batched_encodings = interpolated_encodings.chunk(batches)\n",
    "\n",
    "# Generamos para cada lote\n",
    "images = []\n",
    "for batch in batched_encodings:\n",
    "    noise_batch = torch.cat([noise] * batch.shape[0], dim=0)\n",
    "    images += pipe(prompt_embeds=batch,\n",
    "                   latents=noise_batch,\n",
    "                   num_inference_steps=25).images\n",
    "    \n",
    "export_as_gif(\"doggo-and-fruit-30.gif\", images, rubber_band=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893346d-bf05-4a06-8380-7dae73c7c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "IImage(\"doggo-and-fruit-30.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b58a982-ca63-4c07-a94e-7b2556227380",
   "metadata": {},
   "source": [
    "El gif resultante muestra un cambio m치s claro y coherente entre los dos textos (ten en cuenta que bajamos el numero de pasos). Prueba tus propios textos y experimenta !\n",
    "\n",
    "Incluso podemos extender este concepto para m치s de una imagen. Por ejemplo, podemos interpolar entre cuatro textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf61ee29-7c92-4de6-8bc1-6830193623b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"A watercolor painting of a Golden Retriever at the beach\"\n",
    "prompt_2 = \"A still life DSLR photo of a bowl of fruit\"\n",
    "prompt_3 = \"The eiffel tower in the style of starry night\"\n",
    "prompt_4 = \"An architectural sketch of a skyscraper\"\n",
    "\n",
    "interpolation_steps = 6\n",
    "batch_size = 4\n",
    "batches = (interpolation_steps**2) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17a471-1e1a-4a5f-99b7-57008e49f1d3",
   "metadata": {},
   "source": [
    "Codificamos los textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea4877-d55a-431c-8c67-fdd658497e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_1 = pipe.tokenizer(prompt_1, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "tokenized_2 = pipe.tokenizer(prompt_2, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "tokenized_3 = pipe.tokenizer(prompt_3, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "tokenized_4 = pipe.tokenizer(prompt_4, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "encoding_1 = pipe.text_encoder(tokenized_1.input_ids.to(\"cuda\"))[0][0].cpu().detach().numpy()\n",
    "encoding_2 = pipe.text_encoder(tokenized_2.input_ids.to(\"cuda\"))[0][0].cpu().detach().numpy()\n",
    "encoding_3 = pipe.text_encoder(tokenized_3.input_ids.to(\"cuda\"))[0][0].cpu().detach().numpy()\n",
    "encoding_4 = pipe.text_encoder(tokenized_4.input_ids.to(\"cuda\"))[0][0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcba7e1f-289e-4e66-97c5-06db160506bc",
   "metadata": {},
   "source": [
    "Interpolamos los textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f84ab-f30f-4402-81e8-03c7c4061c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_encodings = torch.from_numpy(np.linspace(\n",
    "    np.linspace(encoding_1, encoding_2, interpolation_steps),\n",
    "    np.linspace(encoding_3, encoding_4, interpolation_steps),\n",
    "    interpolation_steps\n",
    "))\n",
    "\n",
    "interpolated_encodings = interpolated_encodings.reshape(\n",
    "    (interpolation_steps**2, 77, 768)\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bd685f-9228-4aa4-af71-4595fd6d91af",
   "metadata": {},
   "source": [
    "Dividimos en lotes las interpolaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24168bfe-3064-4d51-a8d0-2d385cc1d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_encodings = interpolated_encodings.chunk(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721857fa-34e4-42e8-823b-dbab3e7480be",
   "metadata": {},
   "source": [
    "Procedemos a generar las im치genes interpoladas, lote por lote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dffd6c5-0827-408e-9b05-e80cb4e4c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for batch in batched_encodings:\n",
    "    noise_batch = torch.cat([noise] * batch.shape[0], dim=0)\n",
    "    images += pipe(prompt_embeds=batch,\n",
    "                   latents=noise_batch,\n",
    "                   num_inference_steps=50).images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c6e1a-4993-4004-a764-a8d734c9cf7d",
   "metadata": {},
   "source": [
    "Veamos las im치genes que generamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19259ea6-ade1-4fb4-b15a-d4c36aeea5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpuna_stable_diffusion.utils import image_grid\n",
    "\n",
    "grid = image_grid(images, rows=interpolation_steps, cols=interpolation_steps).resize((800, 800))\n",
    "grid.save(\"4-way-interpolation.jpg\")\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee81af9-74b7-494f-a1f9-decbc9ac206c",
   "metadata": {},
   "source": [
    "Podemos tambi칠n interpolar mientras permitimos que el ruido de difusi칩n varie si es que dejamos de pasar el ruido inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eac66e0-c607-4544-9c8c-270bd60c809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for batch in batched_encodings:\n",
    "    noise_batch = torch.cat([noise] * batch.shape[0], dim=0)\n",
    "    images += pipe(prompt_embeds=batch,\n",
    "                   # latents=noise_batch,\n",
    "                   num_inference_steps=50).images\n",
    "    \n",
    "image_grid(images, rows=interpolation_steps, cols=interpolation_steps).resize((600, 600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9ed48b-4139-43b5-bfe1-8c1831940af5",
   "metadata": {},
   "source": [
    "### Una caminata alrededor de un texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c3893-a728-4bc5-9d93-7957037dbc1c",
   "metadata": {},
   "source": [
    "Nuestro siguiente experimento ser치 ir de caminata alrededor del espacio latente empezando por un punto producido por un texto particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02697b29-3356-4d7b-a736-65ac8e65ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_steps = 30\n",
    "batch_size = 4\n",
    "batches = walk_steps // batch_size\n",
    "step_size = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ec35b-bbae-4447-8f9d-e1a45a5de8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = pipe.tokenizer(\"The Eiffel Tower in the style of starry night\", padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "encoding = pipe.text_encoder(tokenized.input_ids.to(\"cuda\"))[0][0]\n",
    "\n",
    "delta = torch.ones_like(encoding) * step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74e36c-3347-49a6-bcc8-bff39976837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "walked_encodings = []\n",
    "\n",
    "for step_index in range(walk_steps):\n",
    "    walked_encodings.append(encoding + delta * step_index)\n",
    "    \n",
    "walked_encodings = torch.stack(walked_encodings)\n",
    "batched_encodings = walked_encodings.chunk(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e776e-a58c-463b-a638-bfe69df3f085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos de nuevo el ruido\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "noise = torch.randn((1, 4, 64, 64), generator=generator, dtype=torch.float16)\n",
    "\n",
    "# Apagamos el filtro para no tener problemas\n",
    "def dummy(images, **kwargs):\n",
    "    return images, False\n",
    "\n",
    "pipe.safety_checker = dummy\n",
    "\n",
    "images = []\n",
    "for batch in batched_encodings:\n",
    "    noise_batch = torch.cat([noise] * batch.shape[0], dim=0)\n",
    "    images += pipe(prompt_embeds=batch,\n",
    "                   latents=noise_batch,\n",
    "                   num_inference_steps=30).images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9151c8-0396-4aa4-b622-400a94b8eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_as_gif(\"eiffel-tower-starry-night.gif\", images, rubber_band=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25db102-1bf0-4e86-b007-754e51d9826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IImage(\"eiffel-tower-starry-night.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c9c64-ec68-4f37-ba7f-83f2e4bd2675",
   "metadata": {},
   "source": [
    "### Caminata circular sobre el espacio del ruido de difusi칩n para un mismo texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4841af-b65a-4ad8-b12b-a885821dd4c7",
   "metadata": {},
   "source": [
    "Nuestro experimento final ser치 pegarse a un solo texto y explorar la variedad de im치genes que el modelo de difusi칩n puede producir para ese texto. Podemos hacer esto controlando el ruido que es usado para inicial el proceso de difusi칩n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bb8b43-7d9b-43e0-a6b1-19dca5fa93d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "prompt = \"An oil paintings of cows in a field next to a windmill in Holland\"\n",
    "\n",
    "tokenized = pipe.tokenizer(prompt, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "encoding = pipe.text_encoder(tokenized.input_ids.to(\"cuda\"))[0].squeeze()\n",
    "\n",
    "# Generamos de nuevo el ruido\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "noise = torch.randn((1, 4, 64, 64), generator=generator, dtype=torch.float16)\n",
    "\n",
    "walk_steps = 30\n",
    "batch_size = 4\n",
    "batches = walk_steps // batch_size\n",
    "\n",
    "walk_noise_x = torch.randn((4, 64, 64), generator=generator)\n",
    "walk_noise_y = torch.randn((4, 64, 64), generator=generator)\n",
    "\n",
    "walk_scale_x = torch.cos(torch.linspace(0, 2, walk_steps) * math.pi)\n",
    "walk_scale_y = torch.sin(torch.linspace(0, 2, walk_steps) * math.pi)\n",
    "\n",
    "noise_x = torch.tensordot(walk_scale_x, walk_noise_x, dims=0)\n",
    "noise_y = torch.tensordot(walk_scale_y, walk_noise_y, dims=0)\n",
    "\n",
    "noise = (noise_x + noise_y)\n",
    "batched_noise = noise.to(\"cuda\").to(torch.float16).chunk(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d48bb-88dc-42e6-99a2-bac07bb7ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for noise_batch in batched_noise:\n",
    "    prompt_batch = torch.stack([encoding] * noise_batch.shape[0])\n",
    "    images += pipe(prompt_embeds=prompt_batch,\n",
    "                   latents=noise_batch,\n",
    "                   num_inference_steps=30).images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7058b7-df3a-4f54-a1ed-dd2332e19b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_as_gif(\"cows.gif\", images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d880d9-9d7a-46a3-b943-bcc10e01d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IImage(\"cows.gif\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
