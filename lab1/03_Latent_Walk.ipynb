{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad9dda3-3937-485d-ba2c-fc41a6b8d1fe",
   "metadata": {},
   "source": [
    "# Lab 1: Introducción a Stable Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed75b36-4d8b-4df6-b1a9-c514139ddc17",
   "metadata": {},
   "source": [
    "Referencias:\n",
    "\n",
    "- [Stable Diffusion Videos 📽 / nateraw - GitHub ](https://github.com/nateraw/stable-diffusion-videos)\n",
    "- [A walk through latent space with Stable Diffusion - Keras](https://keras.io/examples/generative/random_walks_with_stable_diffusion/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f183285-ce84-4205-8aa9-3522a0db601c",
   "metadata": {},
   "source": [
    "## Parte 3: Generación de Videos con Stable Diffusion 📽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6c2ea4-afeb-4fd1-b22d-e197d3bffe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers accelerate ftfy fpuna-stable-diffusion diffusers==0.14.0 av"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce05809-eb7d-4664-9c9e-8fb91c36d6a8",
   "metadata": {},
   "source": [
    "Los modelos de generación de imágenes aprenden una representación \"latente\" del mundo visual: un vector de baja dimensión donde cada punto se puede convertir en una imagen. Ir desde ese punto latente a una imagen se llama \"decodificación\". En Stable Diffusion 🧨, esto es manejado por el \"decodificador\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e2b12-00c4-4e5d-b461-0f4013c665f4",
   "metadata": {},
   "source": [
    "Esta variedad latente de imágenes es continua e interpolativa, significando que:\n",
    "\n",
    "1. Moverse un poquito en el espacio latente hace que la imagen correspondiente se cambie un poquito (continuidad)\n",
    "2. Para cualquier dos puntos **A** y **B** (por ejemplo, cualquier dos imágenes), es posible moverse de **A** a **B** por un camino donde cada punto intermedio esta también en el espacio latente (por ejemplo, también otra imagen valida). Los puntos intermedios pueden ser llamados \"interpolaciones\" entre las dos imágenes de inicio. \n",
    "\n",
    "Stable Diffusion 🧨 no solo es un modelo de imágenes, sino también, un modelo de lenguaje natural. Posee dos espacios latentes: el espacio de representación de imagen aprendida por el codificador durante el entrenamiento, y el espacio latente del texto que es aprendida usando la combinación de pre-entrenamiento y ajuste duante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b76896c-1d28-4540-9d6d-1650e2e905d0",
   "metadata": {},
   "source": [
    "_Latent space walking_, o, _exploración en el espacio latente_, es el proceso de obtener un punto de muestra en el espacio latente e ir cambiando la representación latente de forma incrementada. Su aplicación más común es generar animaciones donde cada punto muestreado es alimentado al decodificador y es guardado como un fotograma en la animación final.\n",
    "\n",
    "Para representaciones latentes de alta calidad, esto produce animaciones que parecen coherentes. Estas animaciones pueden proveer una cierta intuición del espacio latente, y que puede ultimamente llevar a mejoras en el proceso de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc535177-0094-414f-897d-aa2e47fd25ee",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\"><img src=\"https://keras.io/img/examples/generative/random_walks_with_stable_diffusion/panda2plane.gif\" alt=\"panda a avión\"  width=\"300\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e29b38-437d-4c50-a109-cde39d4a33f0",
   "metadata": {},
   "source": [
    "### Creamos nuestro modelo de Stable Diffusion 🧨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2c7ee-4cf1-4c47-873d-8b826e1e8c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1e71b-7cd4-4931-9d21-8284b78f1834",
   "metadata": {},
   "source": [
    "### Interpolando entre los text prompts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5efdae8-69bf-4002-8c37-0a213a6074f4",
   "metadata": {},
   "source": [
    "En Stable Diffusion, un text prompt es primero codificado en un vector, y esa codificación es luego utilizada para guiar el proceso de difusión. El vector latente tiene un tamaño de 77x768 (es una matriz!), y cuando damos a Stable Diffusion un text prompt, estamos generando imágenes desde solamente un punto en el espacio latente.\n",
    "\n",
    "Para explorar más este espacio latente, podemos interpolar entre dos codificaciones de texto y generar imágenes en esos puntos interpolados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e286b24-5a2a-4513-94f4-3d03cc5bdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "interpolation_steps = 5\n",
    "prompt_1 = \"A watercolor painting of a Golden Retriever at the beach\"\n",
    "prompt_2 = \"A still life DSLR photo of a bowl of fruit\"\n",
    "\n",
    "tokenized_1 = pipe.tokenizer(prompt_1, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "tokenized_2 = pipe.tokenizer(prompt_2, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "encoding_1 = pipe.text_encoder(tokenized_1.input_ids.to(\"cuda\"))[0][0].cpu().detach().numpy()\n",
    "encoding_2 = pipe.text_encoder(tokenized_2.input_ids.to(\"cuda\"))[0][0].cpu().detach().numpy()\n",
    "\n",
    "interpolated_encodings = np.linspace(encoding_1, encoding_2, interpolation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b910ae1-ef12-4cb4-b39b-1d00da7e03d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_encodings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e87d503-e6e3-433c-bf81-00707f0038d2",
   "metadata": {},
   "source": [
    "Una vez que hemos interpolado las codificaciones latentes, podemos generar imágenes desde cualquier punto. Nota que para mantener cierta estabilidad entre las imágenes resultantes, debemos mantener el ruido de difusión constante entre las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da93d3c-e38c-44d1-a3fc-e7fcfc7025b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(12345)\n",
    "noise = torch.randn((1, 4, 64, 64), generator=generator, dtype=torch.float16)\n",
    "noise = torch.cat([noise]*interpolation_steps, dim=0)\n",
    "\n",
    "images = pipe(prompt_embeds=torch.from_numpy(interpolated_encodings).to(\"cuda\"),\n",
    "              latents=noise\n",
    "             ).images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f48126-1a2b-4a5f-81dd-9eb6061218d3",
   "metadata": {},
   "source": [
    "Ahora que generamos algunas imágenes interpoladas, echemos un vistazo!\n",
    "\n",
    "Durante el laboratorio, vamos a estar exportando las secuencias de imagenes como gifs para que puedan verse facilmente con cierto contexto temporal. Para las secuencias de imágenes donde la primera y la ultima imagen no coincide conceptualmente, vamos hacer que se repita el video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff029139-087a-45de-80e4-900e1af1c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpuna_stable_diffusion.utils import image_grid\n",
    "\n",
    "image_grid(images, rows=1, cols=interpolation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98848a4a-124b-4d21-8c2c-a4a0555114d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_as_gif(filename, images, frames_per_second=10, rubber_band=False):\n",
    "    if rubber_band:\n",
    "        images += images[2:-1][::-1]\n",
    "    images[0].save(\n",
    "        filename,\n",
    "        save_all=True,\n",
    "        append_images=images[1:],\n",
    "        duration=1000 // frames_per_second,\n",
    "        loop=0,\n",
    "    )\n",
    "    \n",
    "export_as_gif(\n",
    "    \"doggo-and-fruit-5.gif\",\n",
    "    images,\n",
    "    frames_per_second=2,\n",
    "    rubber_band=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9558007d-fe80-4cde-9723-2188d471173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IImage\n",
    "IImage(\"doggo-and-fruit-5.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02e3262-231f-4e7b-9726-1480345a7f4c",
   "metadata": {},
   "source": [
    "Los resultados pueden parecer sorprendente. Generalmente, interpolar entre prompts produce imágenes coherentes, y demuestran muy de seguido un cambio de concepto progresivo entre los contenidos de los dos textos. Esto es indicativo de un espacio de representación de alta calidad, que refleja cercanamente la estructura natural del mundo visual.\n",
    "\n",
    "Para visualizar mejor, tenemos que hacer una interpolación mucho más fina, utilizando cientos de pasos de interpolación. Para poder mantener el tamaño de las interpolaciones baja (para evitar que la GPU se quede corto de memoria), se requiere lotear manualmente las interpolaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5c0410-1782-4fc1-bf50-f3fa8bfa56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolation_steps = 30\n",
    "batch_size = 3\n",
    "batches = interpolation_steps // batch_size\n",
    "\n",
    "# Generamos de nuevo el ruido\n",
    "generator = torch.Generator().manual_seed(12345)\n",
    "noise = torch.randn((1, 4, 64, 64), generator=generator, dtype=torch.float16)\n",
    "\n",
    "# Generamos las interpolaciones y la dividimos en lotes\n",
    "interpolated_encodings = torch.from_numpy(np.linspace(encoding_1, encoding_2, interpolation_steps))\n",
    "interpolated_encodings = interpolated_encodings.to(\"cuda\") # Hay que llevar a GPU\n",
    "batched_encodings = interpolated_encodings.chunk(batches)\n",
    "\n",
    "# Generamos para cada lote\n",
    "images = []\n",
    "for batch in batched_encodings:\n",
    "    noise_batch = torch.cat([noise] * batch.shape[0], dim=0)\n",
    "    images += pipe(prompt_embeds=batch,\n",
    "                   latents=noise_batch,\n",
    "                   num_inference_steps=25).images\n",
    "    \n",
    "export_as_gif(\"doggo-and-fruit-30.gif\", images, rubber_band=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893346d-bf05-4a06-8380-7dae73c7c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "IImage(\"doggo-and-fruit-30.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b58a982-ca63-4c07-a94e-7b2556227380",
   "metadata": {},
   "source": [
    "El gif resultante muestra un cambio más claro y coherente entre los dos textos (ten en cuenta que bajamos el numero de pasos). Prueba tus propios textos y experimenta !\n",
    "\n",
    "Incluso podemos extender este concepto para más de una imagen. Por ejemplo, podemos interpolar entre cuatro textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf61ee29-7c92-4de6-8bc1-6830193623b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"A watercolor painting of a Golden Retriever at the beach\"\n",
    "prompt_2 = \"A still life DSLR photo of a bowl of fruit\"\n",
    "prompt_3 = \"The eiffel tower in the style of starry night\"\n",
    "prompt_4 = \"An architectural sketch of a skyscraper\"\n",
    "\n",
    "interpolation_steps = 6\n",
    "batch_size = 4\n",
    "batches = (interpolation_steps**2) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17a471-1e1a-4a5f-99b7-57008e49f1d3",
   "metadata": {},
   "source": [
    "Codificamos los textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea4877-d55a-431c-8c67-fdd658497e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_1 = pipe.tokenizer(prompt_1, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "tokenized_2 = pipe.tokenizer(prompt_2, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "tokenized_3 = pipe.tokenizer(prompt_3, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "tokenized_4 = pipe.tokenizer(prompt_4, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "encoding_1 = pipe.text_encoder(tokenized_1.input_ids.to(\"cuda\"))[0][0].cpu().detach().numpy()\n",
    "encoding_2 = pipe.text_encoder(tokenized_2.input_ids.to(\"cuda\"))[0][0].cpu().detach().numpy()\n",
    "encoding_3 = pipe.text_encoder(tokenized_3.input_ids.to(\"cuda\"))[0][0].cpu().detach().numpy()\n",
    "encoding_4 = pipe.text_encoder(tokenized_4.input_ids.to(\"cuda\"))[0][0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcba7e1f-289e-4e66-97c5-06db160506bc",
   "metadata": {},
   "source": [
    "Interpolamos los textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f84ab-f30f-4402-81e8-03c7c4061c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_encodings = torch.from_numpy(np.linspace(\n",
    "    np.linspace(encoding_1, encoding_2, interpolation_steps),\n",
    "    np.linspace(encoding_3, encoding_4, interpolation_steps),\n",
    "    interpolation_steps\n",
    "))\n",
    "\n",
    "interpolated_encodings = interpolated_encodings.reshape(\n",
    "    (interpolation_steps**2, 77, 768)\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bd685f-9228-4aa4-af71-4595fd6d91af",
   "metadata": {},
   "source": [
    "Dividimos en lotes las interpolaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24168bfe-3064-4d51-a8d0-2d385cc1d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_encodings = interpolated_encodings.chunk(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721857fa-34e4-42e8-823b-dbab3e7480be",
   "metadata": {},
   "source": [
    "Procedemos a generar las imágenes interpoladas, lote por lote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dffd6c5-0827-408e-9b05-e80cb4e4c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for batch in batched_encodings:\n",
    "    noise_batch = torch.cat([noise] * batch.shape[0], dim=0)\n",
    "    images += pipe(prompt_embeds=batch,\n",
    "                   latents=noise_batch,\n",
    "                   num_inference_steps=50).images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c6e1a-4993-4004-a764-a8d734c9cf7d",
   "metadata": {},
   "source": [
    "Veamos las imágenes que generamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19259ea6-ade1-4fb4-b15a-d4c36aeea5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpuna_stable_diffusion.utils import image_grid\n",
    "\n",
    "grid = image_grid(images, rows=interpolation_steps, cols=interpolation_steps).resize((800, 800))\n",
    "grid.save(\"4-way-interpolation.jpg\")\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee81af9-74b7-494f-a1f9-decbc9ac206c",
   "metadata": {},
   "source": [
    "Podemos también interpolar mientras permitimos que el ruido de difusión varie si es que dejamos de pasar el ruido inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eac66e0-c607-4544-9c8c-270bd60c809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for batch in batched_encodings:\n",
    "    noise_batch = torch.cat([noise] * batch.shape[0], dim=0)\n",
    "    images += pipe(prompt_embeds=batch,\n",
    "                   # latents=noise_batch,\n",
    "                   num_inference_steps=50).images\n",
    "    \n",
    "image_grid(images, rows=interpolation_steps, cols=interpolation_steps).resize((600, 600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9ed48b-4139-43b5-bfe1-8c1831940af5",
   "metadata": {},
   "source": [
    "### Una caminata alrededor de un texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c3893-a728-4bc5-9d93-7957037dbc1c",
   "metadata": {},
   "source": [
    "Nuestro siguiente experimento será ir de caminata alrededor del espacio latente empezando por un punto producido por un texto particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02697b29-3356-4d7b-a736-65ac8e65ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_steps = 30\n",
    "batch_size = 4\n",
    "batches = walk_steps // batch_size\n",
    "step_size = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ec35b-bbae-4447-8f9d-e1a45a5de8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = pipe.tokenizer(\"The Eiffel Tower in the style of starry night\", padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "encoding = pipe.text_encoder(tokenized.input_ids.to(\"cuda\"))[0][0]\n",
    "\n",
    "delta = torch.ones_like(encoding) * step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74e36c-3347-49a6-bcc8-bff39976837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "walked_encodings = []\n",
    "\n",
    "for step_index in range(walk_steps):\n",
    "    walked_encodings.append(encoding + delta * step_index)\n",
    "    \n",
    "walked_encodings = torch.stack(walked_encodings)\n",
    "batched_encodings = walked_encodings.chunk(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e776e-a58c-463b-a638-bfe69df3f085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos de nuevo el ruido\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "noise = torch.randn((1, 4, 64, 64), generator=generator, dtype=torch.float16)\n",
    "\n",
    "# Apagamos el filtro para no tener problemas\n",
    "def dummy(images, **kwargs):\n",
    "    return images, False\n",
    "\n",
    "pipe.safety_checker = dummy\n",
    "\n",
    "images = []\n",
    "for batch in batched_encodings:\n",
    "    noise_batch = torch.cat([noise] * batch.shape[0], dim=0)\n",
    "    images += pipe(prompt_embeds=batch,\n",
    "                   latents=noise_batch,\n",
    "                   num_inference_steps=30).images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9151c8-0396-4aa4-b622-400a94b8eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_as_gif(\"eiffel-tower-starry-night.gif\", images, rubber_band=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25db102-1bf0-4e86-b007-754e51d9826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IImage(\"eiffel-tower-starry-night.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c9c64-ec68-4f37-ba7f-83f2e4bd2675",
   "metadata": {},
   "source": [
    "### Caminata circular sobre el espacio del ruido de difusión para un mismo texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4841af-b65a-4ad8-b12b-a885821dd4c7",
   "metadata": {},
   "source": [
    "Nuestro experimento final será pegarse a un solo texto y explorar la variedad de imágenes que el modelo de difusión puede producir para ese texto. Podemos hacer esto controlando el ruido que es usado para inicial el proceso de difusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bb8b43-7d9b-43e0-a6b1-19dca5fa93d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "prompt = \"An oil paintings of cows in a field next to a windmill in Holland\"\n",
    "\n",
    "tokenized = pipe.tokenizer(prompt, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "encoding = pipe.text_encoder(tokenized.input_ids.to(\"cuda\"))[0].squeeze()\n",
    "\n",
    "# Generamos de nuevo el ruido\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "noise = torch.randn((1, 4, 64, 64), generator=generator, dtype=torch.float16)\n",
    "\n",
    "walk_steps = 30\n",
    "batch_size = 4\n",
    "batches = walk_steps // batch_size\n",
    "\n",
    "walk_noise_x = torch.randn((4, 64, 64), generator=generator)\n",
    "walk_noise_y = torch.randn((4, 64, 64), generator=generator)\n",
    "\n",
    "walk_scale_x = torch.cos(torch.linspace(0, 2, walk_steps) * math.pi)\n",
    "walk_scale_y = torch.sin(torch.linspace(0, 2, walk_steps) * math.pi)\n",
    "\n",
    "noise_x = torch.tensordot(walk_scale_x, walk_noise_x, dims=0)\n",
    "noise_y = torch.tensordot(walk_scale_y, walk_noise_y, dims=0)\n",
    "\n",
    "noise = (noise_x + noise_y)\n",
    "batched_noise = noise.to(\"cuda\").to(torch.float16).chunk(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d48bb-88dc-42e6-99a2-bac07bb7ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for noise_batch in batched_noise:\n",
    "    prompt_batch = torch.stack([encoding] * noise_batch.shape[0])\n",
    "    images += pipe(prompt_embeds=prompt_batch,\n",
    "                   latents=noise_batch,\n",
    "                   num_inference_steps=30).images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7058b7-df3a-4f54-a1ed-dd2332e19b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_as_gif(\"cows.gif\", images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d880d9-9d7a-46a3-b943-bcc10e01d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IImage(\"cows.gif\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
